# ğŸ¯ VIBR8 - Flickd AI Hackathon Submission

**Smart Tagging & Vibe Classification Engine (Backend MVP)**

## ğŸ“Œ Project Goal

To build a **fully functional backend MVP** for **Flickdâ€™s Smart Tagging & Vibe Classification Engine**, which:

* Detects and classifies fashion products frame-by-frame in videos.
* Extracts aesthetic "vibes" for the entire video using multimodal input (visual, text, transcript).
* Produces structured JSON outputs for downstream UX/UX design, recommendation, and analytics use.

---

## ğŸ—‚ï¸ Repository Structure

```bash
FLICKD_HACK_SUBM
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ video_transcripts/  #.txt transcripts per video
â”‚   â”œâ”€â”€ videos/  #.mp4 videos
â”‚   â”œâ”€â”€ videos-20250604T053958Z-1-001/  #original videos dataset given
â”‚   â”œâ”€â”€ images.csv  #product ids + image urls 
|   â”œâ”€â”€ product_data.xlsx  #product ids + product descriptions
â”‚   â””â”€â”€ vibeslist.json  #vibes considered for classification
â”œâ”€â”€ data_preprocessing/          
â”‚   â”œâ”€â”€ catalog_images.csv/  #downloaded images
â”‚   â”œâ”€â”€ catalog_cleaned.csv  #dataset after preprocessing
â”‚   â”œâ”€â”€ catalog_with_paths.csv  #dataset after image_downloader.py
â”‚   â”œâ”€â”€ catalog.csv  #dataset after catalog_creator.py
|   â”œâ”€â”€ catalog_creator.py
|   â”œâ”€â”€ image_downloader.py 
â”‚   â””â”€â”€ simple_preprocessing.py
â”œâ”€â”€ detection_results/                   
|   â”œâ”€â”€ crops/  #detected product crops
â”‚   â””â”€â”€ detections/
â”œâ”€â”€ frames/ #extracted frames
â”œâ”€â”€ main/                 
â”‚   â”œâ”€â”€ detection.py    # carry out detections
|   â”œâ”€â”€ frame_extraction.py    # extract frames
|   â”œâ”€â”€ matcher.py    # match products
â”‚   â””â”€â”€ vibe_classifier.py #classify vibes per video
â”œâ”€â”€ models/                  
|   â”œâ”€â”€ clip-vit-base-patch32/    
|   â”œâ”€â”€ best.pt    
â”‚   â””â”€â”€ yolov8n.pt
â”œâ”€â”€ outputs/  #Final JSONs with vibe classifications per video
â”œâ”€â”€ yolov8_finetuning/ #personally finetuned yolov8 model for fashion detection with fashionpedia dataset
|   â”œâ”€â”€ yolov8_1.ipynb  
|   â”œâ”€â”€ yolov8_2.ipynb 
â”‚   â””â”€â”€ yolov8_3.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
## WARNINGS:
1. THE clip-vit-base-patch32/ is not present in models/ due to its large size, instead a google drive link to the model folder is given, and you have to add that folder on your local device under the models directory.
Google Drive Link to clip model - https://drive.google.com/drive/folders/1AZ8ml7VHRFitjpnR6R2kRGlKEL3Yqj1Y?usp=drive_link
---

## ğŸš€ How to Run
NOTE- I have used FFMPEG during extraction of frames, you will need to have it installed in your local device and set it as a environment variable. Also be mindful of paths given in the python files, change them as per your relevant file's location.

### ğŸ”§ 1. Setup

```bash
# Clone the repository
git clone https://github.com/arnav-chauhan-kgpian/FLICKD_HACK_SUBM.git
cd FLICKD_HACK_SUBM

# (Optional) Create a virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Required Inputs

Place the following in the `data/` directory:

* `product_data.xlsx`
* `images.csv`
* `video_transcripts/`
* `videos`
* `vibeslist.json`

### 3. Extract Frames(if not already)

``` bash
python main/frame_extraction.py
```
### 4. Run Product Detection (YOLOv8)

```bash
python main/detection.py
```

* Output saved to `/detection_results/`
* Each frame gets cropped product images
  
### 4. Run data preprocessing

```bash
python data_preprocessing/catalog_creator.py
python data_preprocessing/image_downloader.py
python data_preprocessing/simple_preprocessing.py
```

### 5. Run Product Matching

```bash
python main/matcher.py
```

* Matches detected items with cleaned catalog using embedding-based matching
  
### âš™ï¸ 6. Classify Vibes (Per Video)

```bash
python main/vibe_classifier.py
```

* Final vibe classifcations saved to: `/outputs/` as JSONs

---

## ğŸ“Š Sample Output Format (`outputs/<video_id>.json`)

```json
{
  "video_id": "video123",
  "vibes": ["cottagecore", "boho"],
  "confidence_scores": {
    "coquette": 0.12,
    "clean girl": 0.03,
    "cottagecore": 0.92,
    "boho": 0.88,
    ...
  },
  "products": [
    {
      "matched_product_id": "123456",
      "mapped_label": "Dress",
      "confidence": 0.89,
      "crop_path": "crops/video123/Dress_0.jpg"
    },
    ...
  ]
}
```

---

## ğŸ“½ï¸ Demo Video

ğŸ”— ([LOOM DEMO](https://www.loom.com/share/18585635b0ad4a14bcd0a6e3badd7b87?sid=8dd34b30-6915-4ce0-9269-8fa3bc7bdbab))

---

## ğŸ“ Evaluation JSONs

All final outputs are stored under:

```
outputs/
â”œâ”€â”€ video123.json
â”œâ”€â”€ video124.json
â””â”€â”€ ...
```

---

## Vibe Classification Logic 

Priority is:

1. **Exact keyword matches** (title, description, collection)
2. **Semantic similarity** using Sentence Transformers
3. **Color & print features** from product\_tags
4. **Transcript context** (if available)

> All scores are ensemble-weighted, normalized, and final vibes are chosen based on top confidences.

---

## ğŸ“Œ Future Enhancements

* LLM-based caption refinement (Mistral-7B / Mixtral)
* REST API endpoints via FastAPI
* Automatic video â†’ frames extractor
* Streamlit or React-based dashboard

---

## ğŸ¤ Credits

This project was built as a part of the **Flickd AI Hackathon**.
By: \[Arnav Chauhan (3rd Year UG at IIT Kharagpur)]
GitHub: [@arnav-chauhan-kgpian](https://github.com/arnav-chauhan-kgpian)

---
